{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "Answer:\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression. It works like a flowchart, where:\n",
        "\n",
        "Root Node: Represents the entire dataset.\n",
        "Decision Nodes: Represent feature-based splitting rules.\n",
        "Leaf Nodes: Represent the final class or predicted value.\n",
        "\n",
        "In classification, the tree splits the dataset into subsets based on the feature that provides the maximum class separation (using criteria like Gini or Entropy). At prediction time, a data point follows the decision rules until it reaches a leaf node, which assigns a class label.\n",
        "\n",
        "Example:\n",
        "Suppose we classify whether a flower is Setosa or Versicolor.\n",
        "Root Node checks: Petal Length ≤ 2.5 cm?\n",
        "\n",
        "If Yes → Setosa\n",
        "If No → Versicolor\n",
        "Thus, the Decision Tree mimics a series of “if–else” rules for classification.\n",
        "\n",
        "#Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "Answer:\n",
        "1. Gini Impurity:\n",
        "- Formula: Gini = 1 - Σ (p_i)^2\n",
        "- Measures how often a randomly chosen sample would be misclassified.\n",
        "- A Gini of 0 means pure node.\n",
        "\n",
        "2. Entropy:\n",
        "- Formula: Entropy = - Σ p_i * log2(p_i)\n",
        "- Measures the uncertainty or disorder in data.\n",
        "- Entropy = 0 means pure node.\n",
        "\n",
        "Impact on Splits:\n",
        "- Decision Trees choose the split that reduces impurity the most.\n",
        "- Lower impurity → better homogeneity → stronger decision boundary.\n",
        "Example:\n",
        "If a node has 50 samples, with 25 positive and 25 negative:\n",
        "Gini = 0.5 (high impurity)\n",
        "Entropy = 1 (maximum disorder)\n",
        "The tree will try to split this node to reduce impurity.\n",
        "\n",
        "#Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "Answer:\n",
        "- Pre-Pruning:\n",
        "  - Stops tree growth early by setting constraints (max_depth, min_samples_split).\n",
        "  - Advantage: Saves computation, avoids overfitting early.\n",
        "  - Example: Limit tree to depth=3.\n",
        "\n",
        "- Post-Pruning:\n",
        "  - Allows full tree growth, then trims unimportant branches.\n",
        "  - Advantage: More accurate since it evaluates tree first, then removes unnecessary complexity.\n",
        "  - Example: CART pruning removes branches with low impact.\n",
        "\n",
        "Difference Summary:\n",
        "Pre-pruning = “Don't let it grow too big.”\n",
        "Post-pruning = “Grow fully, then cut extra.”\n",
        "\n",
        "#Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:\n",
        "- Information Gain (IG) measures reduction in impurity after a split.\n",
        "- Formula: IG = Entropy(Parent) – Weighted Average(Entropy(Children))\n",
        "\n",
        "Importance:\n",
        "- High IG means a feature separates the data well.\n",
        "- Decision Trees choose the feature with highest IG for splitting.\n",
        "\n",
        "Example:\n",
        "If splitting on Petal Length reduces Entropy from 1.0 to 0.3, IG = 0.7.\n",
        "→ This feature is chosen for the split.\n",
        "\n",
        "#Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "Answer:\n",
        "\n",
        "Applications:\n",
        "\n",
        "Healthcare: Predicting diseases.\n",
        "Finance: Credit risk scoring.\n",
        "Retail: Customer segmentation.\n",
        "Manufacturing: Defect detection.\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Easy to interpret and visualize.\n",
        "Works with both categorical and numerical data.\n",
        "Requires minimal preprocessing.\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Prone to overfitting.\n",
        "Biased toward features with many levels.\n",
        "Sensitive to small changes in data."
      ],
      "metadata": {
        "id": "FAy5QSNuMyy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Info:\n",
        "#● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "#● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).\n",
        "#Question 6: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy and feature importance\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPtEK6YnNPzL",
        "outputId": "5fd3e55d-d0dd-4edc-f94a-742a3e70febc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZ5eWveXMmZr",
        "outputId": "b26f6181-35ad-4c84-c087-5dad889e1900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Tree Accuracy: 1.0\n",
            "Pruned Tree Accuracy (max_depth=3): 1.0\n"
          ]
        }
      ],
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "clf_full.fit(X_train, y_train)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "print(\"Full Tree Accuracy:\", accuracy_score(y_test, clf_full.predict(X_test)))\n",
        "print(\"Pruned Tree Accuracy (max_depth=3):\", accuracy_score(y_test, clf_pruned.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed8oPmXaQdRT",
        "outputId": "4fe2ef70-bcbf-4898-c86a-fefb7a6dc70d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9:Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset (classification target)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target   # y is discrete classes (0,1,2)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV with accuracy scoring\n",
        "grid = GridSearchCV(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(\"Test Accuracy with Best Parameters:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWxEtX5ZQpsc",
        "outputId": "39c74865-2a92-4841-f3d5-9e92ce23431c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Best Cross-Validation Accuracy: 0.9428571428571428\n",
            "Test Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you're working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.Explain the step-by-step process you would follow to:\n",
        "#● Handle the missing values\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters\n",
        "#● Evaluate its performance\n",
        "#And describe what business value this model could provide in the real-world setting\n",
        "Answer:\n",
        "\n",
        "Step 1 - Handle Missing Values\n",
        "\n",
        "Use mean/median for numeric features.\n",
        "Use mode or “Unknown” for categorical features.\n",
        "\n",
        "Step 2 - Encode Categorical Features\n",
        "\n",
        "OneHotEncoding for nominal data.\n",
        "LabelEncoding for ordinal data.\n",
        "\n",
        "Step 3 - Train Decision Tree\n",
        "\n",
        "Use DecisionTreeClassifier with Gini/Entropy.\n",
        "\n",
        "Step 4 - Tune Hyperparameters\n",
        "\n",
        "Use GridSearchCV to optimize max_depth, min_samples_split.\n",
        "\n",
        "Step 5 - Evaluate Model\n",
        "\n",
        "Use Accuracy, Precision, Recall, F1-score, ROC-AUC.\n",
        "Perform Cross-Validation for robustness.\n",
        "\n",
        "Business Value:\n",
        "\n",
        "Early disease prediction → preventive treatment.\n",
        "Helps doctors prioritize high-risk patients.\n",
        "Reduces costs and improves healthcare efficiency."
      ],
      "metadata": {
        "id": "PvMzFKQ3RXYJ"
      }
    }
  ]
}